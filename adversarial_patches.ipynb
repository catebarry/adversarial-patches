{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catebarry/adversarial-patches/blob/main/adversarial_patches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8BzHUUhZS-t"
      },
      "source": [
        "# AIPI 590 - XAI | Assignment 7: Adversarial Patches\n",
        "### Catie Barry\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catebarry/adversarial-patches/blob/main/adversarial_patches.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "In this notebook, we will create an adversarial patch. We will use the Torchvision ResNet34 model trained on a small version of ImageNet to test the patch.\n",
        "\n",
        "Most of this code is replicated from the GitHub cited below. As a creative component, I disguise an adversarial patch for the German short-haired pointer label to look like a sticker for a cafe (with coffee cup logo).\n",
        "\n",
        "\n",
        "**Note:** You will need access to GPU to run this code.\n",
        "\n",
        "**Sources:**\n",
        "- Code and setup in this notebook modified from a [tutorial](https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/adversarial-ai-example-notebooks/adversarial_attacks_patches.ipynb) originally created by Phillip Lippe and modified by Dr. Brinnae Bent.\n",
        "- AI assistance from noted throughout comments and in statement at bottom of notebook."
      ],
      "metadata": {
        "id": "ulegrItzy37Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"adversarial-patches\"\n",
        "git_path = 'https://github.com/catebarry/adversarial-patches.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\" #Add if using requirements.txt\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = ''\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ],
      "metadata": {
        "id": "4ldITFAdt0JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY3eojEEZS-v"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "import math # Import math for radians\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRWq0JvsZS-w"
      },
      "source": [
        "We have again a few download statements. This includes both a dataset, and a few pretrained patches we will use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JduRHZZRZS-w"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alm335hAZS-w"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For our experiments, we will use common CNN architectures trained on the ImageNet dataset (provided by PyTorch's torchvision package). For the results on the website and default on Google Colab, we use a ResNet34."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW_Zr6j5ZS-w"
      },
      "outputs": [],
      "source": [
        "# Load CNN architecture pretrained on ImageNet\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZakluF0wZS-w"
      },
      "source": [
        "To perform adversarial attacks, we also need a dataset to work on. Given that the CNN model has been trained on ImageNet, it is only fair to perform the attacks on data from ImageNet. For this, we provide a small set of pre-processed images from the original ImageNet dataset (note that this dataset is shared under the same [license](http://image-net.org/download-faq) as the original ImageNet dataset). Specifically, we have 5 images for each of the 1000 labels of the dataset. We can load the data below, and create a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WcTBD26ZS-w"
      },
      "outputs": [],
      "source": [
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=8)\n",
        "\n",
        "# Load label names to interpret the label numbers 0 to 999\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqoydYBgZS-x"
      },
      "source": [
        "Before we start with our attacks, we should verify the performance of our model. As ImageNet has 1000 classes, simply looking at the accuracy is not sufficient to tell the performance of a model. A common alternative metric is \"Top-5 accuracy\", which tells us how many times the true label has been within the 5 most-likely predictions of the model. As models usually perform quite well on those, we report the error (1 - accuracy) instead of the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bm4LT16ZS-x"
      },
      "outputs": [],
      "source": [
        "def eval_model(dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if img_func is not None:\n",
        "            imgs = img_func(imgs, labels)\n",
        "        with torch.no_grad():\n",
        "            preds = pretrained_model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DzJmoyhZS-x"
      },
      "outputs": [],
      "source": [
        "_ = eval_model(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly07wViYZS-x"
      },
      "source": [
        "The ResNet34 achives a decent error rate of 4.3% for the top-5 predictions. Next, we can look at some predictions of the model to get more familiar with the dataset. The function below plots an image along with a bar diagram of its predictions. We also prepare it to show adversarial examples for later applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QB3DiAwZS-x"
      },
      "outputs": [],
      "source": [
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "\n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1\n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "\n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFhsaZNZZS-x"
      },
      "source": [
        "Let's visualize a few images below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kliclvDuZS-x"
      },
      "outputs": [],
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEglx5thZS-x"
      },
      "source": [
        "The bar plot on the right shows the top-5 predictions of the model with their class probabilities. We denote the class probabilities with \"confidence\" as it somewhat resembles how confident the network is that the image is of one specific class. Some of the images have a highly peaked probability distribution, and we would expect the model to be rather robust against noise for those. However, we will see below that this is not always the case. Note that all of the images are of fish because the data loader doesn't shuffle the dataset. Otherwise, we would get different images every time we run the notebook, which would make it hard to discuss the results on the static version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "salkFsbMZS-x"
      },
      "source": [
        "## Adversarial Patches\n",
        "\n",
        "Instead of changing every pixel by a little bit, we can change a small part of the image into whatever values we would like. In other words, we will create a small image patch that covers a minor part of the original image but causes the model to confidentially predict a specific class we choose. This form of attack is an even bigger threat in real-world applications than FSGM.\n",
        "\n",
        "### How to train an adversarial patch:\n",
        "- We calculate gradients for the input, and update our adversarial input correspondingly. We do not calculate a gradient for every pixel. Instead, we replace parts of the input image with our patch and then calculate the gradients just for our patch.\n",
        "- Secondly, we don't just do it for one image, but we want the patch to work with any possible image. Hence, we have a whole training loop where we train the patch using SGD.\n",
        "- Lastly, image patches are usually designed to make the model predict a specific class, not just any other arbitrary class except the true label.\n",
        "\n",
        "This following function makes the patch robust to random location, rotation, and scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdCA3Y8_ZS-x"
      },
      "outputs": [],
      "source": [
        "#def place_patch(img, patch):\n",
        "#    for i in range(img.shape[0]):\n",
        "#        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
        "#        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
        "#        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
        "#    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Include random location, rotation, and scale\n",
        "# Generated with help from Claude Sonnet 4.5 on 11/2/25 at 8pm\n",
        "\n",
        "def place_patch(img, patch, rotate_max=25, scale_range=(0.85, 1.15)):\n",
        "    B, C, H, W = img.shape\n",
        "    device = patch.device\n",
        "    img = img.to(device)\n",
        "\n",
        "    # Convert patch parameter to normalized image tensor\n",
        "    patch_norm = patch_forward(patch)\n",
        "\n",
        "    # Random rotation and scale (same for entire batch)\n",
        "    angle = random.uniform(-rotate_max, rotate_max)\n",
        "    scale = random.uniform(scale_range[0], scale_range[1])\n",
        "\n",
        "    # Apply transformations using torchvision (differentiable)\n",
        "    patch_transformed = TF.rotate(patch_norm, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
        "\n",
        "    # Scale by resizing\n",
        "    ph, pw = patch_norm.shape[-2:]\n",
        "    new_h = int(ph * scale)\n",
        "    new_w = int(pw * scale)\n",
        "    patch_transformed = TF.resize(patch_transformed, [new_h, new_w], interpolation=TF.InterpolationMode.BILINEAR)\n",
        "\n",
        "    pt_h, pt_w = patch_transformed.shape[-2:]\n",
        "\n",
        "    # Random location (same for entire batch)\n",
        "    max_h_off = max(0, H - pt_h)\n",
        "    max_w_off = max(0, W - pt_w)\n",
        "    h_off = random.randint(0, max_h_off) if max_h_off > 0 else 0\n",
        "    w_off = random.randint(0, max_w_off) if max_w_off > 0 else 0\n",
        "\n",
        "    h_end = min(h_off + pt_h, H)\n",
        "    w_end = min(w_off + pt_w, W)\n",
        "    actual_h = h_end - h_off\n",
        "    actual_w = w_end - w_off\n",
        "\n",
        "    # Place patch using mask (no in-place operations)\n",
        "    mask = torch.zeros((1, C, H, W), device=device)\n",
        "    mask[:, :, h_off:h_end, w_off:w_end] = 1.0\n",
        "\n",
        "    patch_full = torch.zeros((1, C, H, W), device=device)\n",
        "    patch_full[:, :, h_off:h_end, w_off:w_end] = patch_transformed[:, :actual_h, :actual_w].unsqueeze(0)\n",
        "\n",
        "    result = img * (1 - mask) + patch_full * mask\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "eYGARgzTKiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15XjfldDZS-y"
      },
      "source": [
        "The patch itself will be an `nn.Parameter` whose values are in the range between $-\\infty$ and $\\infty$. Images are, however, naturally limited in their range, and thus we write a small function that maps the parameter into the image value range of ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqHPOu_cZS-y"
      },
      "outputs": [],
      "source": [
        "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
        "def patch_forward(patch):\n",
        "    means = TENSOR_MEANS.to(patch.device)\n",
        "    std = TENSOR_STD.to(patch.device)\n",
        "    # Ensure patch is on the same device as TENSOR_MEANS and TENSOR_STD\n",
        "    # patch = patch.to(TENSOR_MEANS.device)\n",
        "\n",
        "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
        "    patch = (torch.tanh(patch) + 1 - 2 * means) / (2 * std)\n",
        "    return patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLLRu60ZS-y"
      },
      "source": [
        "Before looking at the actual training code, we can write a small evaluation function. We evaluate the success of a patch by how many times we were able to fool the network into predicting our target class. A simple function for this is implemented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7a-ocjlZS-y"
      },
      "outputs": [],
      "source": [
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
        "            for _ in range(4):\n",
        "                patch_img = place_patch(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
        "                # as we would not \"fool\" the model into predicting those\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp/counter\n",
        "    top5 = tp_5/counter\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYN4QZiDZS-y"
      },
      "source": [
        "Finally, we can look at the training loop. Given a model to fool, a target class to design the patch for, and a size $k$ of the patch in the number of pixels, we first start by creating a parameter of size $3\\times k\\times k$. These are the only parameters we will train, and the network itself remains untouched. We use a simple SGD optimizer with momentum to minimize the classification loss of the model given the patch in the image. While we first start with a very high loss due to the good initial performance of the network, the loss quickly decreases once we start changing the patch. In the end, the patch will represent patterns that are characteristic of the class. For instance, if we would want the model to predict a \"goldfish\" in every image, we would expect the pattern to look somewhat like a goldfish. Over the iterations, the model finetunes the pattern and, hopefully, achieves a high fooling accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM4jXmU4ZS-y"
      },
      "outputs": [],
      "source": [
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    # Leave a small set of images out to check generalization\n",
        "    # In most of our experiments, the performance on the hold-out data points\n",
        "    # was as good as on the training set. Overfitting was little possible due\n",
        "    # to the small size of the patches.\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    # Create parameter and optimizer\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "    #patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1]), requires_grad=True)\n",
        "    # ADDED THE FOLLOWING LINE OF CODE\n",
        "    patch = nn.Parameter(init_param.clone().to(device), requires_grad=True)\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        batch_idx = 0\n",
        "\n",
        "        for img, _ in t:\n",
        "            img = place_patch(img, patch)\n",
        "            #img = img.to(device)\n",
        "            pred = model(img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "\n",
        "            ##### ADDED CODE FOR REGULARIZERS, generated by Claude Sonnet 4.5 on 11/2/25 at 8pm\n",
        "\n",
        "            patch_norm = patch_forward(patch)\n",
        "            # Convert to pixel space for regularization\n",
        "            patch_pixels = torch.clamp(\n",
        "                patch_norm * TENSOR_STD.to(patch_norm.device) + TENSOR_MEANS.to(patch_norm.device),\n",
        "                0.0, 1.0\n",
        "            )\n",
        "\n",
        "            lambda_tv = 2e-2      # Smoothness (reduce jagged edges)\n",
        "            lambda_dark = 5e-2    # Keep it darker\n",
        "            target_brightness = 0.35  # Slightly darker than middle gray\n",
        "\n",
        "            # Total variation (smoothness)\n",
        "            tv_h = (patch_pixels[:,1:,:] - patch_pixels[:,:-1,:]).abs().mean()\n",
        "            tv_w = (patch_pixels[:,:,1:] - patch_pixels[:,:,:-1]).abs().mean()\n",
        "            tv = tv_h + tv_w\n",
        "\n",
        "            # Darkness penalty\n",
        "            dark = F.relu(patch_pixels.mean() - target_brightness)\n",
        "\n",
        "            loss = loss + lambda_tv * tv + lambda_dark * dark\n",
        "\n",
        "            #####\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            #loss.mean().backward()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            batch_idx += 1\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "\n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR8T3q8zZS-y"
      },
      "source": [
        "To get some experience with what to expect from an adversarial patch attack, we want to train multiple patches for different classes. As the training of a patch can take one or two minutes on a GPU, we have provided a couple of pre-trained patches including their results on the full dataset. The results are saved in a JSON file, which is loaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQFHkMKcZS-y"
      },
      "outputs": [],
      "source": [
        "# Load evaluation results of the pretrained patches\n",
        "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
        "json_results = {}\n",
        "if os.path.isfile(json_results_file):\n",
        "    with open(json_results_file, \"r\") as f:\n",
        "        json_results = json.load(f)\n",
        "\n",
        "# If you train new patches, you can save the results via calling this function\n",
        "def save_results(patch_dict):\n",
        "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t\n",
        "                                   for t in patch_dict[cname][psize][\"results\"]]\n",
        "                           for psize in patch_dict[cname]}\n",
        "                   for cname in patch_dict}\n",
        "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCr-bSg6ZS-y"
      },
      "source": [
        "Additionally, we implement a function to train and evaluate patches for a list of classes and patch sizes. The pretrained patches include the classes *toaster*, *goldfish*, *school bus*, *lipstick*, and *pineapple*. We chose the classes arbitrarily to cover multiple domains (animals, vehicles, fruits, devices, etc.). We trained each class for three different patch sizes: $32\\times32$ pixels, $48\\times48$ pixels, and $64\\times64$ pixels. We can load them in the two cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odzEel_eZS-y"
      },
      "outputs": [],
      "source": [
        "def get_patches(class_names, patch_sizes):\n",
        "    result_dict = dict()\n",
        "\n",
        "    # Loop over all classes and patch sizes\n",
        "    for name in class_names:\n",
        "        result_dict[name] = dict()\n",
        "        for patch_size in patch_sizes:\n",
        "            c = label_names.index(name)\n",
        "            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
        "            # Load patch if pretrained file exists, otherwise start training\n",
        "            if not os.path.isfile(file_name):\n",
        "                patch, val_results = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n",
        "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
        "                torch.save(patch, file_name)\n",
        "            else:\n",
        "                patch = torch.load(file_name)\n",
        "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
        "            if name in json_results:\n",
        "                results = json_results[name][str(patch_size)]\n",
        "\n",
        "            else:\n",
        "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)\n",
        "            # Store results and the patches in a dict for better access\n",
        "            result_dict[name][patch_size] = {\n",
        "                \"results\": results,\n",
        "                \"patch\": patch\n",
        "            }\n",
        "\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code initializes the patch to an image of a coffee logo on brown background."
      ],
      "metadata": {
        "id": "XOJNiYh7e5QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this block of code was generated with help from AI on 11/2/25 at 9:10pm\n",
        "\n",
        "# Load coffee_logo.png from repo and convert to init_param\n",
        "from PIL import Image\n",
        "\n",
        "PATCH_SIZE = 64\n",
        "USER_PNG_PATH = \"coffee_logo.png\"\n",
        "assert os.path.exists(USER_PNG_PATH), f\"File not found: {USER_PNG_PATH}\"\n",
        "\n",
        "img = Image.open(USER_PNG_PATH).convert(\"RGB\") # Convert to RGB directly\n",
        "\n",
        "ph = pw = PATCH_SIZE\n",
        "\n",
        "# Resize the image to the patch size\n",
        "img = img.resize((pw, ph), Image.LANCZOS)\n",
        "\n",
        "# Convert to 0..1 floats\n",
        "arr = np.array(img).astype(np.float32) / 255.0  # shape (H,W,3)\n",
        "\n",
        "# Use existing ImageNet mean/std variables from the notebook; fall back if not present\n",
        "try:\n",
        "    MEAN = np.array(NORM_MEAN)\n",
        "    STD  = np.array(NORM_STD)\n",
        "except Exception:\n",
        "    MEAN = np.array([0.485,0.456,0.406], dtype=np.float32)\n",
        "    STD  = np.array([0.229,0.224,0.225], dtype=np.float32)\n",
        "\n",
        "# Invert the notebook's patch_forward mapping to get pre-tanh parameter\n",
        "norm_pix = (arr - MEAN[None,None,:]) / STD[None,None,:]\n",
        "tanh_p = 2.0 * (STD[None,None,:] * norm_pix) - 1.0 + 2.0 * MEAN[None,None,:]\n",
        "tanh_p = np.clip(tanh_p, -0.999, 0.999)\n",
        "init_param_np = 0.5 * np.log((1.0 + tanh_p) / (1.0 - tanh_p))\n",
        "init_param = torch.tensor(np.transpose(init_param_np.astype(np.float32), (2,0,1)))  # (3,H,W)\n",
        "\n",
        "# Visual check (optional)\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(img.resize((256,256))) # Display the resized image directly\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"init_param ready, shape:\", init_param.shape)"
      ],
      "metadata": {
        "id": "5tD0zsoyeaup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4TTixjtZS-y"
      },
      "outputs": [],
      "source": [
        "class_names = ['toaster', 'goldfish', 'school bus', 'lipstick', 'pineapple', 'German short-haired pointer']\n",
        "patch_sizes = [64] #[32, 48, 64]\n",
        "\n",
        "patch_dict = get_patches(class_names, patch_sizes)\n",
        "# save_results(patch_dict) # Uncomment if you add new class names and want to save the new results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU3wlAFnZS-1"
      },
      "source": [
        "Before looking at the quantitative results, we can actually visualize the patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jyArm1wZS-1"
      },
      "outputs": [],
      "source": [
        "def show_patches():\n",
        "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
        "    ax = np.atleast_2d(ax)\n",
        "    for c_idx, cname in enumerate(class_names):\n",
        "        for p_idx, psize in enumerate(patch_sizes):\n",
        "            patch = patch_dict[cname][psize][\"patch\"]\n",
        "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
        "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
        "            ax[p_idx][c_idx].imshow(patch)\n",
        "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
        "            ax[p_idx][c_idx].axis('off')\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    plt.show()\n",
        "show_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UUAdFgxZS-1"
      },
      "source": [
        "You can see that the German short-haired pointer patch is dark and shows the outline of the coffee mug--this is due to the initialization as the coffee cup image with brown background and the regularization terms to encourage smooth, less noisy patches and darkness penalties.\n",
        "We also trained it to be robust to random rotations, scaling, and locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shef0TCcZS-1"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Some HTML code to increase font size in the following table -->\n",
        "<style>\n",
        "th {font-size: 120%;}\n",
        "td {font-size: 120%;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqqfK9_hZS-1"
      },
      "outputs": [],
      "source": [
        "import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_table(top_1=True):\n",
        "    i = 0 if top_1 else 1\n",
        "    table = [[name] + [f\"{(100.0 * patch_dict[name][psize]['results'][i]):4.2f}%\" for psize in patch_sizes]\n",
        "             for name in class_names]\n",
        "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Class name\"] + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RtCYaqyZS-1"
      },
      "source": [
        "First, we will create a table of top-1 accuracy, meaning that how many images have been classified with the target class as highest prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT4ECFf3ZS-1"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IKn4LjdZS-1"
      },
      "source": [
        "The German short-haired pointer patch doesn't perform as well in top-1 accuracy, likely because it is put under much more constraints than the other patches--making it less likely to perform consistently as the top target class.\n",
        "\n",
        "Let's also take a look at the top-5 accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7I8OdJUZS-1"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PotaBfhiZS-1"
      },
      "source": [
        "The patch performs much better in the top-5 accuracy, showing that it is still rather robust despite the constraints put on it to disguise the patch.\n",
        "\n",
        "Finally, let's create some example visualizations of the patch attack in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_uVI6x6ZS-1"
      },
      "outputs": [],
      "source": [
        "def perform_patch_attack(patch):\n",
        "    patch_batch = exmp_batch.clone()\n",
        "    patch_batch = place_patch(patch_batch, patch)\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch.to(device))\n",
        "    for i in range(1,17,5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKnGZhKLZS-1"
      },
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['German short-haired pointer'][64]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Assistance Statement\n",
        "Portions of this notebook were developed with assistance from Anthropic's Claude Sonnet 4.5 (November 2025). Claude was used for debugging and code development assistance (for robustness augmentations and disguising the patch). All generated code and explanations were reviewed, tested, and edited to ensure correctness and alignment with assignment requirements.\n",
        "\n",
        "Google Gemini 2.5 Flash was used to generate the coffee mug png on 11/2/25 at 6pm.\n",
        "\n",
        "Specific dates/times and prompts to generate code blocks are listed below:\n",
        "| Date/Time | Summary | Code Added / Modified |\n",
        "|:-----------|:---------|:---------------------|\n",
        "| **Nov 2, 2025<br>~11:00 PM EST** | Implement random transformations for patch placement | Created `place_patch()` with random rotation, scale, and location augmentation |\n",
        "| **Nov 2, 2025<br>~11:10 PM EST** | Fix gradient flow and device mismatches | Updated `place_patch()` to handle CUDA/CPU correctly, removed non-differentiable per-image transforms, and replaced in-place ops with safe blending |\n",
        "| **Nov 2, 2025<br>~11:00 PM EST** | Validate gradient flow and training stability | Simplified patch placement (fixed location) to isolate bugs; confirmed gradients update the patch correctly |\n",
        "| **Nov 2, 2025<br>~11:00 PM EST** | Finalize robust transformation implementation | Reintroduced differentiable rotation/scale using `torchvision.transforms` (`TF.rotate`, `TF.resize`) |\n",
        "| **Nov 2, 2025<br>~11:00 PM EST** | Add visual regularization for realistic patch appearance | Added Total Variation (TV) and darkness losses to training loop to encourage smooth, dark, printable patch |\n",
        "| **Nov 2, 2025<br>~11:00 PM EST** | Implement coffee-logo initialization for disguise | Loaded `coffee_logo.png`, composited over brown badge, converted to `init_param` (pre-tanh) for patch initialization |\n",
        "\n",
        "---\n",
        "\n",
        "Key Issues Resolved:\n",
        "- Fixed device mismatches (CPU â†” GPU)  \n",
        "- Restored proper gradient flow through patch parameter  \n",
        "- Eliminated non-differentiable image transforms and in-place ops  \n",
        "- Ensured evaluation tests patch robustness at multiple locations  \n",
        "- Added realistic **coffee-logo initialization** for disguise  \n",
        "- Added **TV + darkness regularization** for visual plausibility and printability"
      ],
      "metadata": {
        "id": "FLZBhjGC0lGc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TktaoexsZS-3"
      },
      "source": [
        "## References\n",
        "\n",
        "This tutorial was originally created by Phillip Lippe.\n",
        "[![View notebooks on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb)  \n",
        "\n",
        "[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" ICLR 2015.\n",
        "\n",
        "[2] Hendrik Metzen, Jan, et al. \"Universal adversarial perturbations against semantic image segmentation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n",
        "\n",
        "[3] Anant Jain. \"Breaking neural networks with adversarial attacks.\" [Blog post](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa) 2019."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}